{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d73GALobn03X",
        "outputId": "670290ab-f9d6-4262-b8a0-b454bec6a247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import BertConfig\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "Vdh3glbOoaZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "training_data = pd.read_csv('/content/train_data_text_ver1.csv')\n",
        "val_data = pd.read_csv('/content/val_data_text_ver1.csv')\n",
        "test_data = pd.read_csv('/content/test_data_text_ver1.csv')"
      ],
      "metadata": {
        "id": "BtmayParoj8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = list(training_data['prompt_text'])\n",
        "train_labels = list(training_data['approved'])\n",
        "val_texts =  list(val_data['prompt_text'])\n",
        "val_labels =  list(val_data['approved'])"
      ],
      "metadata": {
        "id": "D2exzrFn65nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have text in a list and corresponding labels in another list\n",
        "\n",
        "\n",
        "texts = train_texts + val_texts\n",
        "labels = train_labels + val_labels\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for text in texts:\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,  # You can adjust this to your desired sequence length\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"tf\",\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids.append(encoding[\"input_ids\"])\n",
        "    attention_masks.append(encoding[\"attention_mask\"])\n",
        "\n",
        "input_ids = tf.concat(input_ids, axis=0)\n",
        "attention_masks = tf.concat(attention_masks, axis=0)\n",
        "\n",
        "# Create a TensorFlow dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": input_ids, \"attention_mask\": attention_masks}, labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JA1Lngroc51",
        "outputId": "fae12b43-3c8b-49bd-ee06-d558cb0f2fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a dataset of 80% for training, 10% for validation, and 10% for testing\n",
        "dataset_size = len(texts)\n",
        "train_size = dataset_size\n",
        "train_dataset = dataset.take(train_size)"
      ],
      "metadata": {
        "id": "bsB1Ygwdo-UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained BERT model for sequence classification\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Define optimizer, loss function, and metrics\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset.shuffle(1000).batch(16),\n",
        "    epochs=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICsav57wpGI-",
        "outputId": "a4fbc9aa-3341-4f70-ec98-c41fd7d4d817"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "411/411 [==============================] - 231s 429ms/step - loss: 0.6489 - accuracy: 0.6264\n",
            "Epoch 2/5\n",
            "411/411 [==============================] - 176s 429ms/step - loss: 0.5500 - accuracy: 0.7286\n",
            "Epoch 3/5\n",
            "411/411 [==============================] - 176s 429ms/step - loss: 0.4792 - accuracy: 0.7803\n",
            "Epoch 4/5\n",
            "411/411 [==============================] - 176s 429ms/step - loss: 0.4254 - accuracy: 0.8136\n",
            "Epoch 5/5\n",
            "411/411 [==============================] - 176s 429ms/step - loss: 0.3780 - accuracy: 0.8336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have test_texts as a list of text samples\n",
        "test_texts = list(test_data['prompt_text'])\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for text in test_texts:\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,  # Adjust this based on your training settings\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"tf\",\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids.append(encoding[\"input_ids\"])\n",
        "    attention_masks.append(encoding[\"attention_mask\"])\n",
        "\n",
        "input_ids = tf.concat(input_ids, axis=0)\n",
        "attention_masks = tf.concat(attention_masks, axis=0)\n"
      ],
      "metadata": {
        "id": "e6XLJivFvuaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict({\"input_ids\": input_ids, \"attention_mask\": attention_masks})\n",
        "\n",
        "# Extract the predicted labels (class probabilities) from the model's output\n",
        "predicted_labels = tf.argmax(predictions.logits, axis=-1).numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy2I8vwPGs80",
        "outputId": "71ca04bd-5dbb-4ac4-ad6f-11847c3ebffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52/52 [==============================] - 18s 277ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['approved'] = predicted_labels"
      ],
      "metadata": {
        "id": "bw1o67ZDPnUO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[['id','approved']].to_csv('submission_bert.csv',index=False)"
      ],
      "metadata": {
        "id": "shTbyqa4XLAQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nqmKzUuUXYsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}